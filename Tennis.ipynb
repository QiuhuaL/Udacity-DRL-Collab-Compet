{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "from maddpg_agent import Agent\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis_Windows_x86_64\\Tennis.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agents' performance, if they select actions at random with each time step.  A window should pop up that allows you to observe the agents.\n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agents are able to use their experiences to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 1: 0.09000000171363354\n",
      "Score (max over agents) from episode 2: 0.0\n",
      "Score (max over agents) from episode 3: 0.0\n",
      "Score (max over agents) from episode 4: 0.0\n",
      "Score (max over agents) from episode 5: 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the MADDPG Agent\n",
    "Now it's your turn to train your own agent to solve the environment! When training the environment, set train_mode=True, so that the line for resetting the environment looks like the following:\n",
    "\n",
    "env_info = env.reset(train_mode=True)[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def maddpg(n_episodes=4000, max_t=1000,  maxlen=100, solved_score_limit=0.5, train_mode=True):\n",
    "    \"\"\"Multi-Agent Deep Deterministic Policy Gradient (MADDPG) \n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        train_mode (bool)     : if 'True' set environment to training mode\n",
    "        maxlen(int): the window size to take the average scores \n",
    "        solved_score (float)  : minimun average score over maxlen consecutive episodes, and over all agents\n",
    "    \"\"\"\n",
    "    scores_window = deque(maxlen = maxlen)  # mean score list over maxlen episodes\n",
    "    scores_all = []  # all scores for for agent and each episode\n",
    "    max_scores_agents = [] # the max score of the two agents in each episode\n",
    "    mean_score_episodes = [] # the max score of the two agents averaged over continuous episodes\n",
    "    \n",
    "   \n",
    "    solved = 0 \n",
    "    solved_episode = 0 \n",
    "    solved_score = 0\n",
    "    best_episode = 0\n",
    "    best_score = 0\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode = train_mode)[brain_name] # reset the environment\n",
    "        states = env_info.vector_observations     # get the current state for each agent\n",
    "        scores = np.zeros(num_agents)            \n",
    "        for t in range(max_t):\n",
    "            actions = agent.act(states, add_noise=True)  # select the action with adding noise, for each agent\n",
    "            env_info = env.step(actions)[brain_name]     # send the action to the environment\n",
    "            next_states = env_info.vector_observations   # get the next state for each agent\n",
    "            rewards = env_info.rewards                   # get the reward for each agent\n",
    "            dones = env_info.local_done                  # see if episode has finished\n",
    "            #print(states)        \n",
    "            #print(actions)       \n",
    "            #print(next_states) \n",
    "            #print(dones)\n",
    "            #print(rewards)       \n",
    "                           \n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "            states = next_states\n",
    "            scores += rewards            \n",
    "            if np.any(dones):\n",
    "                break \n",
    "             \n",
    "        scores_all.append(scores)     \n",
    "        max_scores_agents.append(np.max(scores))\n",
    "        scores_window.append(np.max(scores))   \n",
    "        average_score = np.mean(scores_window)  \n",
    "        mean_score_episodes.append(average_score)\n",
    "        \n",
    "        print('\\rEpisode {}\\t Max Score: {:.2f} \\t Average Score: {:.2f}'.format(i_episode, np.max(scores), average_score))\n",
    "        if average_score>=solved_score_limit and train_mode and solved == 0:\n",
    "            print('\\n Target Score Reached in {:d} episodes!\\t Average Score: {:.2f}'.format(i_episode,average_score))\n",
    "            torch.save(agent.return_actors()[0].state_dict(), 'solved_checkpoint_actor1.pth')\n",
    "            torch.save(agent.return_actors()[1].state_dict(), 'solved_checkpoint_actor2.pth')\n",
    "            torch.save(agent.return_critic().state_dict(), 'solved_checkpoint_critic.pth')\n",
    "            solved = 1\n",
    "            solved_episode = i_episode\n",
    "            solved_score = average_score\n",
    "            torch.save({\n",
    "            'solved_episode': solved_episode,\n",
    "            'solved_score': solved_score,\n",
    "            'scores_all': scores_all,\n",
    "            'max_scores_agents': max_scores_agents,\n",
    "            'mean_score_episodes': mean_score_episodes,\n",
    "            }, 'traning_solved_scores.pt')        \n",
    "        \n",
    "        if average_score > best_score and train_mode:\n",
    "            print('\\nBest Score Reached in {:d} episodes!\\t Best Average Score: {:.2f}'.format(i_episode,average_score))\n",
    "            torch.save(agent.return_actors()[0].state_dict(), 'bestscore_checkpoint_actor1.pth')\n",
    "            torch.save(agent.return_actors()[1].state_dict(), 'bestscore_checkpoint_actor2.pth')\n",
    "            torch.save(agent.return_critic().state_dict(), 'bestscore_checkpoint_critic.pth')\n",
    "            best_episode = i_episode\n",
    "            best_score = average_score\n",
    "            torch.save({\n",
    "            'best_episode': best_episode,\n",
    "            'best_score': best_score,\n",
    "            'scores_all': scores_all,\n",
    "            'max_scores_agents': max_scores_agents,\n",
    "            'mean_score_episodes': mean_score_episodes,\n",
    "            }, 'training_bestscores.pt')\n",
    "    \n",
    "        torch.save({\n",
    "            'i_episode': i_episode,\n",
    "            'solved_episode': solved_episode,\n",
    "            'solved_score': solved_score,\n",
    "            'best_episode': best_episode,\n",
    "            'best_score': best_score,\n",
    "            'scores_all': scores_all,\n",
    "            'max_scores_agents': max_scores_agents,\n",
    "            'mean_score_episodes': mean_score_episodes,\n",
    "            }, 'training_all_scores.pt')\n",
    "    print('\\n\\nTraining Done...')\n",
    "    if solved == 1:\n",
    "        print('\\nTarget Score Reached in {:d} episodes,\\t Average Score: {:.2f}'.format(solved_episode,solved_score))\n",
    "    print('\\nBest Score Reached in {:d} episodes,\\t Best Average Score: {:.2f}'.format(best_episode,best_score))\n",
    "    return scores_all,  max_scores_agents, mean_score_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Plot the Training Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 2\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 3\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 4\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 5\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 6\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 7\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 8\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 9\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 10\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 11\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 12\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 13\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 14\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 15\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 16\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 17\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 18\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 19\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 20\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 21\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 22\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 23\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 24\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 25\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 26\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 27\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 28\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 29\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 30\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 31\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 32\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 33\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 34\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 35\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 36\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 37\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 38\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 39\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 40\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 41\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 42\t Max Score: 0.10 \t Average Score: 0.00\n",
      "\n",
      "Best Score Reached in 42 episodes!\t Best Average Score: 0.00\n",
      "Episode 43\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 44\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 45\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 46\t Max Score: 0.10 \t Average Score: 0.00\n",
      "\n",
      "Best Score Reached in 46 episodes!\t Best Average Score: 0.00\n",
      "Episode 47\t Max Score: 0.10 \t Average Score: 0.01\n",
      "\n",
      "Best Score Reached in 47 episodes!\t Best Average Score: 0.01\n",
      "Episode 48\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 49\t Max Score: 0.10 \t Average Score: 0.01\n",
      "\n",
      "Best Score Reached in 49 episodes!\t Best Average Score: 0.01\n",
      "Episode 50\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 51\t Max Score: 0.10 \t Average Score: 0.01\n",
      "\n",
      "Best Score Reached in 51 episodes!\t Best Average Score: 0.01\n",
      "Episode 52\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 53\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 54\t Max Score: 0.10 \t Average Score: 0.01\n",
      "\n",
      "Best Score Reached in 54 episodes!\t Best Average Score: 0.01\n",
      "Episode 55\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 56\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 57\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 58\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 59\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 60\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 61\t Max Score: 0.10 \t Average Score: 0.01\n",
      "\n",
      "Best Score Reached in 61 episodes!\t Best Average Score: 0.01\n",
      "Episode 62\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 63\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 64\t Max Score: 0.10 \t Average Score: 0.01\n",
      "\n",
      "Best Score Reached in 64 episodes!\t Best Average Score: 0.01\n",
      "Episode 65\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 66\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 67\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 68\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 69\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 70\t Max Score: 0.09 \t Average Score: 0.01\n",
      "\n",
      "Best Score Reached in 70 episodes!\t Best Average Score: 0.01\n",
      "Episode 71\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 72\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 73\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 74\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 75\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 76\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 77\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 78\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 79\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 80\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 81\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 82\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 83\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 84\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 85\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 86\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 87\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 88\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 89\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 90\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 91\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 92\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 93\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 94\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 95\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 96\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 97\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 98\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 99\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 100\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 101\t Max Score: 0.10 \t Average Score: 0.01\n",
      "Episode 102\t Max Score: 0.10 \t Average Score: 0.01\n",
      "Episode 103\t Max Score: 0.10 \t Average Score: 0.01\n",
      "Episode 104\t Max Score: 0.10 \t Average Score: 0.01\n",
      "\n",
      "Best Score Reached in 104 episodes!\t Best Average Score: 0.01\n",
      "Episode 105\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 106\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 107\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 108\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 109\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 110\t Max Score: 0.10 \t Average Score: 0.01\n",
      "\n",
      "Best Score Reached in 110 episodes!\t Best Average Score: 0.01\n",
      "Episode 111\t Max Score: 0.10 \t Average Score: 0.01\n",
      "\n",
      "Best Score Reached in 111 episodes!\t Best Average Score: 0.01\n",
      "Episode 112\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 113\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 114\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 115\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 116\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 117\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 118\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 119\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 120\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 121\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 122\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 123\t Max Score: 0.10 \t Average Score: 0.02\n",
      "\n",
      "Best Score Reached in 123 episodes!\t Best Average Score: 0.02\n",
      "Episode 124\t Max Score: 0.10 \t Average Score: 0.02\n",
      "\n",
      "Best Score Reached in 124 episodes!\t Best Average Score: 0.02\n",
      "Episode 125\t Max Score: 0.00 \t Average Score: 0.02\n",
      "Episode 126\t Max Score: 0.00 \t Average Score: 0.02\n",
      "Episode 127\t Max Score: 0.00 \t Average Score: 0.02\n",
      "Episode 128\t Max Score: 0.00 \t Average Score: 0.02\n",
      "Episode 129\t Max Score: 0.00 \t Average Score: 0.02\n",
      "Episode 130\t Max Score: 0.00 \t Average Score: 0.02\n",
      "Episode 131\t Max Score: 0.00 \t Average Score: 0.02\n",
      "Episode 132\t Max Score: 0.00 \t Average Score: 0.02\n",
      "Episode 133\t Max Score: 0.00 \t Average Score: 0.02\n",
      "Episode 134\t Max Score: 0.00 \t Average Score: 0.02\n",
      "Episode 135\t Max Score: 0.00 \t Average Score: 0.02\n",
      "Episode 136\t Max Score: 0.00 \t Average Score: 0.02\n",
      "Episode 137\t Max Score: 0.00 \t Average Score: 0.02\n",
      "Episode 138\t Max Score: 0.00 \t Average Score: 0.02\n",
      "Episode 139\t Max Score: 0.00 \t Average Score: 0.02\n",
      "Episode 140\t Max Score: 0.00 \t Average Score: 0.02\n",
      "Episode 141\t Max Score: 0.00 \t Average Score: 0.02\n",
      "Episode 142\t Max Score: 0.00 \t Average Score: 0.02\n",
      "Episode 143\t Max Score: 0.00 \t Average Score: 0.02\n",
      "Episode 144\t Max Score: 0.00 \t Average Score: 0.02\n",
      "Episode 145\t Max Score: 0.00 \t Average Score: 0.02\n",
      "Episode 146\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 147\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 148\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 149\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 150\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 151\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 152\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 153\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 154\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 155\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 156\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 157\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 158\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 159\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 160\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 161\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 162\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 163\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 164\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 165\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 166\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 167\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 168\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 169\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 170\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 171\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 172\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 173\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 174\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 175\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 176\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 177\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 178\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 179\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 180\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 181\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 182\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 183\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 184\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 185\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 186\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 187\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 188\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 189\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 190\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 191\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 192\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 193\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 194\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 195\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 196\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 197\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 198\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 199\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 200\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 201\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 202\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 203\t Max Score: 0.00 \t Average Score: 0.01\n",
      "Episode 204\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 205\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 206\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 207\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 208\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 209\t Max Score: 0.00 \t Average Score: 0.00\n",
      "Episode 210\t Max Score: 0.00 \t Average Score: 0.00\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(state_size=state_size, action_size=action_size, random_seed=100530, num_agents=num_agents)\n",
    "scores,  max_scores_agents, mean_score_episodes = maddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load('training_bestscores.pt')\n",
    "scores = checkpoint['scores_all']\n",
    "max_scores_agents = checkpoint['max_scores_agents']\n",
    "mean_score_episodes =  checkpoint['mean_score_episodes']\n",
    "plt.figure(figsize=(20,10))\n",
    "scores=np.array(scores)\n",
    "m,n=scores.shape\n",
    "plt.plot(np.arange(1, len(scores)+1), scores[:,0], 'g--')  \n",
    "plt.plot(np.arange(1, len(scores)+1), scores[:,1], 'c-.')    \n",
    "plt.plot(np.arange(1, len(scores)+1), max_scores_agents, 'bo')\n",
    "plt.plot(np.arange(1, len(scores)+1), mean_score_episodes,'r', linewidth=3.0)\n",
    "plt.grid()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Training scores')\n",
    "plt.legend(('Scores of the Agent 1', 'Scores of the Agent 2','Max score of the 2 Agents', 'Mean-Max Score 100 Episodes '), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Watch a Smart Agent to Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# solved check point\n",
    "agent.actors_local[0].load_state_dict(torch.load('solved_checkpoint_actor1.pth'))\n",
    "agent.actors_local[1].load_state_dict(torch.load('solved_checkpoint_actor2.pth'))\n",
    "agent.critic_local.load_state_dict(torch.load('solved_checkpoint_critic.pth'))\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "play_episodes  =  10\n",
    "for i in range(1, play_episodes+1):\n",
    "    while True:\n",
    "        actions = agent.act(states)                        # select an action (for each agent)\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# best score checkpoint\n",
    "agent.actors_local[0].load_state_dict(torch.load('bestscore_checkpoint_actor1.pth'))\n",
    "agent.actors_local[1].load_state_dict(torch.load('bestscore_checkpoint_actor2.pth'))\n",
    "agent.critic_local.load_state_dict(torch.load('bestscore_checkpoint_critic.pth'))\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "play_episodes  =  10\n",
    "for i in range(1, play_episodes+1):\n",
    "    while True:\n",
    "        actions = agent.act(states)                        # select an action (for each agent)\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
